{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:53:16.948257700Z",
     "start_time": "2024-02-21T11:53:15.571641600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import comet_ml as comet\n",
    "\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Any, DefaultDict\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API = comet.API()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa7d31",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "**NOTE:** It is expected that the Notebooks are run **inside VS Code** as it allows the pathing for `task_configs` to work. If it is run outside a VS Code instance, please adjust the following line:\n",
    "\n",
    "```py\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "```\n",
    "\n",
    "The cell down below has the following configuration attributes, which might need adjustment depending on changes of the experimental design\n",
    "\n",
    "- `metrics`: Inside this dictionary the keys represent the actual names of the metric, as they are displayed on `comet`, while the values are simply just given the according type that will be fetched from online.\n",
    "  \n",
    "- `parameters`: The parameters describe general experimental setup information, which were passed as arguments upon execution\n",
    "  \n",
    "- `task_names`: The task names represent the data sets upon which the Outlier Detection Strategies were trained on\n",
    "  \n",
    "- `task_configs`: The task configs represent the path to the configuration files of the `task names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072bf8989680c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:28.203914300Z",
     "start_time": "2024-02-21T11:54:28.153936800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"AutoFilter_Chen_Like_HTL Count\": float,\n",
    "    \"AutoFilter_Chen_Like_avg_duration\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (random replacement)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"HDBScanFilter_HTL Count\": float,\n",
    "    \"HDBScanFilter_avg_duration\": float,\n",
    "    \"HDBScanFilter_medF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (random replacement)\": float,\n",
    "    \"HDBScanFilter_avgF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (random replacement)\": float,\n",
    "\n",
    "    \"IsolationForestFilter_HTL Count\": float,\n",
    "    \"IsolationForestFilter_avg_duration\": float,\n",
    "    \"IsolationForestFilter_avgF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (random replacement)\": float,\n",
    "    \"IsolationForestFilter_medF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"LocalOutlierFactorFilter_HTL Count\": float,\n",
    "    \"LocalOutlierFactorFilter_avg_duration\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (random replacement)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"LoserFilter_Plain_HTL Count\": float,\n",
    "    \"LoserFilter_Plain_avg_duration\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (random replacement)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"SingleStepEntropy_SimplePseudo_HTL Count\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avg_duration\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (random replacement)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (random replacement)\": float,\n",
    "\n",
    "}\n",
    "parameters = {\n",
    "    \"strategy_name\": str,\n",
    "    \"filter_strategy_name\": str,\n",
    "    \"seed\": int,\n",
    "    \"task\": str,\n",
    "}\n",
    "\n",
    "task_names = [\n",
    "    \"ag-news\",\n",
    "    \"banking77\"\n",
    "    # \"dbpedia\",\n",
    "    # \"fnc_one\",\n",
    "    # \"imdb\",\n",
    "    # \"mnli\",\n",
    "    # \"qnli\",\n",
    "    # \"rotten-tomatoes\",\n",
    "    # \"sst2\",\n",
    "    # \"trec-coarse\",\n",
    "    # \"trec\",\n",
    "    # \"wiki-talk\",\n",
    "    # \"yelp\"\n",
    "]\n",
    "\n",
    "version = \"x\"\n",
    "task_names = [version+t for t in task_names]\n",
    "\n",
    "# This gets the location of the Notebook, needs VSCode to be executed correctly\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "\n",
    "BASE_PATH = Path(notebook_name).parent\n",
    "CONFIGS_PATH = BASE_PATH.parent / 'Configs' / 'Tasks'\n",
    "\n",
    "task_configs = {\n",
    "    \"ag-news\": CONFIGS_PATH / \"ag_news.json\",\n",
    "    \"banking77\" : CONFIGS_PATH / \"bank77.json\",\n",
    "    \"dbpedia\": CONFIGS_PATH / \"dbpedia.json\",\n",
    "    \"fnc1\": CONFIGS_PATH / \"fnc_one.json\",\n",
    "    \"imdb\": CONFIGS_PATH / \"imdb.json\",\n",
    "    \"mnli\": CONFIGS_PATH / \"mnli.json\",\n",
    "    \"qnli\": CONFIGS_PATH / \"qnli.json\",\n",
    "    \"rotten-tomatoes\": CONFIGS_PATH / \"rotten_tomatoes.json\",\n",
    "    \"sst2\": CONFIGS_PATH / \"sst2.json\",\n",
    "    \"trec-coarse\": CONFIGS_PATH / \"trec_coarse.json\",\n",
    "    \"trec\": CONFIGS_PATH / \"trec.json\",\n",
    "    \"wiki-talk\": CONFIGS_PATH / \"wiki_talk.json\",\n",
    "    \"yelp\": CONFIGS_PATH / \"yelp.json\"\n",
    "}\n",
    "\n",
    "seed_count = 10 # How many different seeds do we expect?\n",
    "\n",
    "filter_names = [\"HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c8726a3f36ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:29.916779600Z",
     "start_time": "2024-02-21T11:54:29.886500800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_metric_value(metrics_used: List[Dict[str, Any]], metric_name: str) -> str | float:\n",
    "    \"\"\"\n",
    "    Extracts the value of a specified metric from a list of metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics_used (List[Dict[str, Any]]): A list of dictionaries containing metrics information.\n",
    "        metric_name (str): The name of the metric to extract.\n",
    "\n",
    "    Returns:\n",
    "        str | float: The value of the specified metric.\n",
    "    \"\"\"\n",
    "    metrics_dict = [entry for entry in metrics_used if entry.get(\"metricName\") == metric_name]\n",
    "    return metrics_dict[0][\"metricValue\"]\n",
    "\n",
    "def extract_paremeter_value(parameters_used: List[Dict[str, Any]], parameter_name: str) -> str | float:\n",
    "    \"\"\"\n",
    "    Extracts the current value of a specified parameter from a list of parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters_used (List[Dict[str, Any]]): A list of dictionaries containing parameter information.\n",
    "        parameter_name (str): The name of the parameter to extract.\n",
    "\n",
    "    Returns:\n",
    "        str | float: The current value of the specified parameter.\n",
    "    \"\"\"\n",
    "    parameters_dict = [entry for entry in parameters_used if entry.get(\"name\") == parameter_name]\n",
    "    return parameters_dict[0][\"valueCurrent\"]\n",
    "\n",
    "\n",
    "def load_experiment_data(experiment: comet.APIExperiment) -> DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Loads and organizes experiment data, including metrics, parameters, and assets.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing experiment data.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]: A nested dictionary with tasks, seeds, and experiment data.\n",
    "    \"\"\"\n",
    "    data = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "    experiment_parameters = experiment.get_parameters_summary()\n",
    "    task = extract_paremeter_value(experiment_parameters, \"task\")\n",
    "    seed = extract_paremeter_value(experiment_parameters, \"seed\")\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_used = experiment.get_metrics()\n",
    "    for metric_name in metrics.keys():\n",
    "        metric_value = extract_metric_value(metrics_used, metric_name)\n",
    "        if not metric_value:\n",
    "            return None\n",
    "        else:\n",
    "            metrics_dict[metric_name] = metric_value\n",
    "\n",
    "    params_dict = {}\n",
    "    for param_name in parameters.keys():\n",
    "        param_value = extract_paremeter_value(experiment_parameters, param_name)\n",
    "        if not experiment_parameters:\n",
    "            return None\n",
    "        else:\n",
    "            params_dict[param_name] = param_value\n",
    "\n",
    "    assets = download_assets(experiment, task, seed)\n",
    "\n",
    "    data[task][seed][\"metrics\"].update(metrics_dict)\n",
    "    data[task][seed][\"parameters\"].update(params_dict)\n",
    "    data[task][seed][\"assets\"].update(assets)\n",
    "    return data\n",
    "\n",
    "def convert_dataframe_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the data types of DataFrame columns based on predefined mappings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with converted column data types.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col in metrics:\n",
    "            df[col] = df[col].astype(metrics[col])\n",
    "        elif col in parameters:\n",
    "            df[col] = df[col].astype(parameters[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_data_frames(experiment_data: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]]) -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    \"\"\"\n",
    "    Creates DataFrames from experiment data and converts their data types.\n",
    "\n",
    "    Args:\n",
    "        experiment_data (List[DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]]): A list of nested dictionaries containing experiment data.\n",
    "\n",
    "    Returns:\n",
    "         DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]: A nested dictionary with pd.DataFrames for each section of the experiment data.\n",
    "    \"\"\"\n",
    "    for index, entry in enumerate(experiment_data):\n",
    "        for task, seed_dict in entry.items():\n",
    "            for seed, section_dict in seed_dict.items():\n",
    "                for section, hyperparameters_dict in section_dict.items():\n",
    "                    df = pd.DataFrame.from_dict([hyperparameters_dict])\n",
    "                    df_converted = convert_dataframe_types(df=df)\n",
    "\n",
    "                    entry[task][seed][section] = df_converted\n",
    "\n",
    "        experiment_data[index] = entry\n",
    "\n",
    "    return experiment_data\n",
    "\n",
    "def load_workspace_data(project_name: str) -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    \"\"\"\n",
    "    Loads experiment data for a specific project from the Comet workspace.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): The name of the project to load data from.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]: A dictionarty of nested dictionaries containing experiment data and DataFrames.\n",
    "    \"\"\"\n",
    "    experiments = API.get(workspace=\"outlier-detection\", project_name=project_name)\n",
    "    experiment_data = []\n",
    "    for exp in experiments:\n",
    "        try:\n",
    "            loaded_data = load_experiment_data(exp)\n",
    "            if loaded_data is not None:\n",
    "                experiment_data.append(loaded_data)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    experiment_data = create_data_frames(experiment_data=experiment_data)\n",
    "    return experiment_data\n",
    "\n",
    "\n",
    "def download_assets(experiment, task: str, seed: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Downloads and saves the assets of an experiment, filtering out unnecessary files.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing the experiment data.\n",
    "        task (str): The task name associated with the experiment.\n",
    "        seed (str): The seed value associated with the experiment.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: A dictionary of downloaded assets, loaded as NumPy arrays.\n",
    "    \"\"\"\n",
    "    assets = experiment.get_asset_list()\n",
    "    filtered_assets = [\n",
    "        asset for asset in assets\n",
    "        if \"durations\" not in asset[\"fileName\"] and not asset[\"fileName\"].endswith(\".py\")\n",
    "    ]\n",
    "\n",
    "    asset_ids = []\n",
    "    for asset in filtered_assets:\n",
    "        asset_ids.append((asset[\"fileName\"], asset[\"assetId\"]))\n",
    "\n",
    "    assets_downloaded = {}\n",
    "    for file_name, idx in asset_ids:\n",
    "        asset_data = experiment.get_asset(idx)\n",
    "        asset_path = Path(f\"./cache/assets/{task}/{seed}/{file_name}\")\n",
    "        asset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(asset_path, \"wb\") as f:\n",
    "            f.write(asset_data)\n",
    "        assets_downloaded[file_name[:-4]] = np.load(asset_path)\n",
    "    return assets_downloaded\n",
    "\n",
    "\n",
    "# df = load_workspace_data(\"xag-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e558f86ffdd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:31.285414Z",
     "start_time": "2024-02-21T11:54:31.252474400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_all_seeds(data: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]], task_name: str):\n",
    "    collected_seeds = []\n",
    "\n",
    "    for data_dict in data:\n",
    "        seed = next(iter(data_dict[task_name]))\n",
    "        collected_seeds.append(seed)\n",
    "\n",
    "    return np.array(collected_seeds, dtype=int)\n",
    "\n",
    "def get_filter_strategy_name(group: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]], task_name: str):\n",
    "    data = group[0]\n",
    "    seed = next(iter(data[task_name]))\n",
    "    filter_strategy = data[task_name][seed][\"parameters\"][\"filter_strategy_name\"].iat[0]\n",
    "\n",
    "    return filter_strategy\n",
    "\n",
    "def process_group(group: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]], task_name: str, workspace_name: str):\n",
    "\n",
    "    seeds = collect_all_seeds(group, task_name=task_name)\n",
    "    missing_seeds = np.setdiff1d(np.arange(42, 42 + seed_count), seeds)\n",
    "    print(len(missing_seeds))\n",
    "\n",
    "    # Take the first element of the list, look up the first seed (because every experiment has the same outlier filter strategy), and then take the result from the first row at column 'filter_strategy_name'\n",
    "    filter_strategy = get_filter_strategy_name(group, task_name)\n",
    "    df = pd.DataFrame([{\"seed\":seed, \"filter\":filter_strategy, \"task\": task_name} for seed in missing_seeds])\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(workspace_name: str):\n",
    "    loaded_data = load_workspace_data(workspace_name)\n",
    "    task = next(iter(loaded_data[0])) # Get only back the first entry of the list, as all elements inside it are going to have the same task name\n",
    "    processed_data = process_group(loaded_data, task, workspace_name)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# load_data(\"xag-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c0bc58138c54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T12:53:02.254155500Z",
     "start_time": "2024-02-21T11:54:32.912064200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for task in tqdm(task_names):\n",
    "    try:\n",
    "        data = load_data(task)\n",
    "        results.append(data)\n",
    "    except:\n",
    "        # missing_seeds = np.arange(42,42+seed_count)\n",
    "        # df = pd.DataFrame([{\"seed\":seed, \"filter\":filter_names[0], \"task\": task} for seed in missing_seeds])\n",
    "        print(f\"{task}: Missing\")\n",
    "\n",
    "\n",
    "missing_experiments = pd.concat(results)\n",
    "\n",
    "missing_experiments = missing_experiments[missing_experiments[\"filter\"].isin(filter_names)]\n",
    "missing_experiments.to_csv(\"missing_std_experiments.csv\")\n",
    "missing_experiments # 941, 856, 848, 762, 747, (856, 715, 650, 607, 516, 503, 384, 370, 322, 290, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ed05b234aadca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T14:30:05.879416900Z",
     "start_time": "2024-02-01T14:30:04.230197800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def add_boxplots(results, filter, l:list):\n",
    "    # Adds BoxPlot to the graph\n",
    "    # Adds multiple medians to the graph\n",
    "    for f in [\"NoneR\", filter, \"NoneE\"]:\n",
    "        data = []\n",
    "        for task in task_names:\n",
    "            data += list(results[task][\"f1s\"][f])\n",
    "        l.append(data)\n",
    "        \n",
    "\n",
    "\n",
    "l = []\n",
    "filter_names_ = [\n",
    "    # \"AutoFilter_LSTM_SIMPLE\",\n",
    "    # \"AutoFilter_LSTM\",\n",
    "    \"AutoFilter_Chen_Like\",\n",
    "    \"LoserFilter_Plain\",\n",
    "    # \"LoserFilter_Optimized_Pseudo_Labels\",\n",
    "    # \"LoserFilter_SSL_Variety\",\n",
    "    # \"TeachingFilter\",\n",
    "    # \"TeachingFilter_WOW\",\n",
    "    # \"TeachingFilter_Smooth\",\n",
    "    \"SingleStepEntropy_SimplePseudo\",\n",
    "    # \"SingleStepEntropy\",\n",
    "    \"HDBScanFilter\",\n",
    "    \"IsolationForestFilter\",\n",
    "    \"LocalOutlierFactorFilter\"\n",
    "]\n",
    "\n",
    "filter_names_clean = {\n",
    "    # \"LoserFilter_SSL_Variety\": \"EXPANDED DSM\", \n",
    "    \"LoserFilter_Plain\": \"SIMPLE DSM\",\n",
    "    # \"LoserFilter_Optimized_Pseudo_Labels\": \"MC DSM\", \n",
    "    \"AutoFilter_Chen_Like\": \"SEMANTIC AE\", \n",
    "    # \"AutoFilter_LSTM\": \"LSTM ENSEMBLE AE\",\n",
    "    # \"AutoFilter_LSTM_SIMPLE\": \"SIMPLE LSTM AE\", \n",
    "    # \"SingleStepEntropy\": \"MC SS\", \n",
    "    \"SingleStepEntropy_SimplePseudo\": \"SIMPLE SS\",\n",
    "    # \"TeachingFilter\": \"SIMPLE LE\", \n",
    "    # \"TeachingFilter_Smooth\": \"SMOOTH LE\", \n",
    "    \"TeachingFilter_WOW\": \"HIGH ENTROPY LE\",\n",
    "    \"HDBScanFilter\" : \"HDBScan\",\n",
    "    \"IsolationForestFilter\": \"IsolationForest\",\n",
    "    \"LocalOutlierFactorFilter\": \"LocalOutlierFactor\"\n",
    "    }\n",
    "\n",
    "for filter in filter_names_:\n",
    "    add_boxplots(results, filter, l)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "intra_group_dist = 0.75\n",
    "inter_group_dist = 1.5\n",
    "positions = [i*intra_group_dist + (i//3)*inter_group_dist for i in range(len(l))]\n",
    "bp = ax.boxplot(l, showfliers=False, positions=positions, patch_artist=True)\n",
    "colors = ['#FF7F50', '#7cda9e', '#8fdeff']\n",
    "for i, patch in enumerate(bp['boxes']):\n",
    "    patch.set_facecolor(colors[i%3])\n",
    "\n",
    "\n",
    "for i, median in enumerate(bp['medians']):\n",
    "    median_x, median_y = median.get_xydata()[1]  # Get the median line's X and Y data\n",
    "    # Hide the median line\n",
    "    median.set_visible(False)\n",
    "    # Plot a diamond marker at the median position\n",
    "    offset = 0.25\n",
    "    ax.plot(median_x-offset, median_y, 'd', color='#082239', markersize=3)\n",
    "\n",
    "tick_positions = [np.mean(positions[(i*3):(i*3)+3])-len(filter_names_clean[f])*0.18 for i, f in enumerate(filter_names_)]\n",
    "\n",
    "plt.xticks(tick_positions, [filter_names_clean[f] for f in filter_names_], rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"endresults.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea530926c4efd2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T14:44:03.295575500Z",
     "start_time": "2024-02-01T14:43:55.053909700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for task in task_names:\n",
    "    def add_boxplots(results, filter, l:list):\n",
    "        # Adds BoxPlot to the graph\n",
    "        # Adds multiple medians to the graph\n",
    "        for f in [\"NoneR\", filter, \"NoneE\"]:\n",
    "            data = []\n",
    "            data += list(results[task][\"f1s\"][f])\n",
    "            l.append(data)\n",
    "            \n",
    "    \n",
    "    \n",
    "    l = []\n",
    "    filter_names_ = [\n",
    "        \"AutoFilter_LSTM_SIMPLE\",\n",
    "        \"AutoFilter_LSTM\",\n",
    "        \"AutoFilter_Chen_Like\",\n",
    "        \"LoserFilter_Plain\",\n",
    "        \"LoserFilter_Optimized_Pseudo_Labels\",\n",
    "        \"LoserFilter_SSL_Variety\",\n",
    "        \"TeachingFilter\",\n",
    "        \"TeachingFilter_WOW\",\n",
    "        \"TeachingFilter_Smooth\",\n",
    "        \"SingleStepEntropy_SimplePseudo\",\n",
    "        \"SingleStepEntropy\",\n",
    "    ]\n",
    "    \n",
    "    filter_names_clean = {\n",
    "        \"LoserFilter_SSL_Variety\": \"EXPANDED DSM\", \n",
    "        \"LoserFilter_Plain\": \"SIMPLE DSM\",\n",
    "        \"LoserFilter_Optimized_Pseudo_Labels\": \"MC DSM\", \n",
    "        \"AutoFilter_Chen_Like\": \"SEMANTIC AE\", \n",
    "        \"AutoFilter_LSTM\": \"LSTM ENSEMBLE AE\",\n",
    "        \"AutoFilter_LSTM_SIMPLE\": \"SIMPLE LSTM AE\", \n",
    "        \"SingleStepEntropy\": \"MC SS\", \n",
    "        \"SingleStepEntropy_SimplePseudo\": \"SIMPLE SS\",\n",
    "        \"TeachingFilter\": \"SIMPLE LE\", \n",
    "        \"TeachingFilter_Smooth\": \"SMOOTH LE\", \n",
    "        \"TeachingFilter_WOW\": \"HIGH ENTROPY LE\"}\n",
    "    \n",
    "    for filter in filter_names_:\n",
    "        add_boxplots(results, filter, l)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    intra_group_dist = 0.75\n",
    "    inter_group_dist = 1.5\n",
    "    positions = [i*intra_group_dist + (i//3)*inter_group_dist for i in range(len(l))]\n",
    "    bp = ax.boxplot(l, showfliers=False, positions=positions, patch_artist=True)\n",
    "    colors = ['#FF7F50', '#7cda9e', '#8fdeff']\n",
    "    for i, patch in enumerate(bp['boxes']):\n",
    "        patch.set_facecolor(colors[i%3])\n",
    "    \n",
    "    \n",
    "    for i, median in enumerate(bp['medians']):\n",
    "        median_x, median_y = median.get_xydata()[1]  # Get the median line's X and Y data\n",
    "        # Hide the median line\n",
    "        median.set_visible(False)\n",
    "        # Plot a diamond marker at the median position\n",
    "        offset = 0.25\n",
    "        ax.plot(median_x-offset, median_y, 'd', color='#082239', markersize=3)\n",
    "    \n",
    "    tick_positions = [np.mean(positions[(i*3):(i*3)+3])-len(filter_names_clean[f])*0.18 for i, f in enumerate(filter_names_)]\n",
    "    \n",
    "    plt.xticks(tick_positions, [filter_names_clean[f] for f in filter_names_], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"endresults-{task}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03d3ef80819d8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the figure and axes for the boxplots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Incremental Boxplots')\n",
    "\n",
    "# This list will hold different datasets for individual boxplots\n",
    "datasets = []\n",
    "\n",
    "# Assuming you have a mechanism to add datasets one at a time\n",
    "for i in range(1, 6):  # Example loop to simulate adding 5 datasets incrementally\n",
    "    # Simulate generating or loading a new dataset\n",
    "    new_data = np.random.normal(loc=i, scale=0.5, size=100)\n",
    "    datasets.append(new_data)  # Add the new dataset to the list\n",
    "    \n",
    "# Clear the axes for fresh plot (optional if you want to redraw the boxplots)\n",
    "ax.clear()\n",
    "# Plot all the current datasets as individual boxplots\n",
    "ax.boxplot(datasets)\n",
    "ax.set_title('Incremental Boxplots')\n",
    "plt.draw()  # Redraw the plot with the new data\n",
    "plt.pause(0.5)  # Pause to visually confirm the addition, adjust or remove as needed\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9d191b06d81d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import deepsig\n",
    "import pandas as pd\n",
    "aso_test = {}\n",
    "for filter_name in filter_names:\n",
    "    data = []\n",
    "    task_aso = {}\n",
    "    for task in task_names:\n",
    "        htl = results[task][\"f1s\"][\"NoneR\"]\n",
    "        no_htl = results[task][\"f1s\"][filter_name]\n",
    "        better = deepsig.aso(no_htl, htl, seed=42)\n",
    "        task_aso[task+\"_no_htl_is_better\"] = better\n",
    "    aso_test[filter_name] = task_aso\n",
    "    \n",
    "pd.DataFrame(aso_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d72026ac224c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Creating a new DataFrame for the pairs\n",
    "paired_data = pd.DataFrame({\n",
    "    'Value': np.concatenate([data['A'], data['B'], data['C'], data['D'], data['E'], data['F']]),\n",
    "    'Variable': np.concatenate([np.repeat('A', 100), np.repeat('B', 100), \n",
    "                                np.repeat('C', 100), np.repeat('D', 100),\n",
    "                                np.repeat('E', 100), np.repeat('F', 100)]),\n",
    "    'Pair': np.concatenate([np.repeat('Pair AB', 200), np.repeat('Pair CD', 200), np.repeat('Pair EF', 200)])\n",
    "})\n",
    "\n",
    "# Plotting the paired boxplots\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.boxplot(x=\"Pair\", y=\"Value\", hue=\"Variable\", data=paired_data, palette=\"Set3\")\n",
    "\n",
    "plt.title(\"Paired Boxplots\")\n",
    "plt.xlabel(\"Pairs\")\n",
    "plt.ylabel(\"Values\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
