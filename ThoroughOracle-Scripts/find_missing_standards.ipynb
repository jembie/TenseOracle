{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:53:16.948257700Z",
     "start_time": "2024-02-21T11:53:15.571641600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import comet_ml as comet\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Any, DefaultDict, Callable\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "API = comet.API()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa7d31",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "**NOTE:** It is expected that the Notebooks are run **inside VS Code** as it allows the pathing for `task_configs` to work. If it is run outside a VS Code instance, please adjust the following line:\n",
    "\n",
    "```py\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "```\n",
    "\n",
    "The cell down below has the following configuration attributes, which might need adjustment depending on changes of the experimental design\n",
    "\n",
    "- `metrics`: Inside this dictionary the keys represent the actual names of the metric, as they are displayed on `comet`, while the values are simply just given the according type that will be fetched from online.\n",
    "  \n",
    "- `parameters`: The parameters describe general experimental setup information, which were passed as arguments upon execution\n",
    "  \n",
    "- `task_names`: The task names represent the data sets upon which the Outlier Detection Strategies were trained on\n",
    "  \n",
    "- `task_configs`: The task configs represent the path to the configuration files of the `task names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072bf8989680c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:28.203914300Z",
     "start_time": "2024-02-21T11:54:28.153936800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"AutoFilter_Chen_Like_HTL Count\": float,\n",
    "    \"AutoFilter_Chen_Like_avg_duration\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (random replacement)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"HDBScanFilter_HTL Count\": float,\n",
    "    \"HDBScanFilter_avg_duration\": float,\n",
    "    \"HDBScanFilter_medF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (random replacement)\": float,\n",
    "    \"HDBScanFilter_avgF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (random replacement)\": float,\n",
    "\n",
    "    \"IsolationForestFilter_HTL Count\": float,\n",
    "    \"IsolationForestFilter_avg_duration\": float,\n",
    "    \"IsolationForestFilter_avgF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (random replacement)\": float,\n",
    "    \"IsolationForestFilter_medF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"LocalOutlierFactorFilter_HTL Count\": float,\n",
    "    \"LocalOutlierFactorFilter_avg_duration\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (random replacement)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"LoserFilter_Plain_HTL Count\": float,\n",
    "    \"LoserFilter_Plain_avg_duration\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (random replacement)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (random replacement)\": float,\n",
    "\n",
    "    \"SingleStepEntropy_SimplePseudo_HTL Count\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avg_duration\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (random replacement)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (random replacement)\": float,\n",
    "\n",
    "}\n",
    "parameters = {\n",
    "    \"strategy_name\": str,\n",
    "    \"filter_strategy_name\": str,\n",
    "    \"seed\": int,\n",
    "    \"task\": str,\n",
    "}\n",
    "\n",
    "task_names = [\n",
    "    \"ag-news\",\n",
    "    \"banking77\"\n",
    "    # \"dbpedia\",\n",
    "    # \"fnc_one\",\n",
    "    # \"imdb\",\n",
    "    # \"mnli\",\n",
    "    # \"qnli\",\n",
    "    # \"rotten-tomatoes\",\n",
    "    # \"sst2\",\n",
    "    # \"trec-coarse\",\n",
    "    # \"trec\",\n",
    "    # \"wiki-talk\",\n",
    "    # \"yelp\"\n",
    "]\n",
    "\n",
    "version = \"x\"\n",
    "task_names = [version + t for t in task_names]\n",
    "\n",
    "# This gets the location of the Notebook, needs VSCode to be executed correctly\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "\n",
    "BASE_PATH = Path(notebook_name).parent\n",
    "CONFIGS_PATH = BASE_PATH.parent / \"Configs\" / \"Tasks\"\n",
    "\n",
    "task_configs = {\n",
    "    \"ag-news\": CONFIGS_PATH / \"ag_news.json\",\n",
    "    \"banking77\": CONFIGS_PATH / \"bank77.json\",\n",
    "    \"dbpedia\": CONFIGS_PATH / \"dbpedia.json\",\n",
    "    \"fnc1\": CONFIGS_PATH / \"fnc_one.json\",\n",
    "    \"imdb\": CONFIGS_PATH / \"imdb.json\",\n",
    "    \"mnli\": CONFIGS_PATH / \"mnli.json\",\n",
    "    \"qnli\": CONFIGS_PATH / \"qnli.json\",\n",
    "    \"rotten-tomatoes\": CONFIGS_PATH / \"rotten_tomatoes.json\",\n",
    "    \"sst2\": CONFIGS_PATH / \"sst2.json\",\n",
    "    \"trec-coarse\": CONFIGS_PATH / \"trec_coarse.json\",\n",
    "    \"trec\": CONFIGS_PATH / \"trec.json\",\n",
    "    \"wiki-talk\": CONFIGS_PATH / \"wiki_talk.json\",\n",
    "    \"yelp\": CONFIGS_PATH / \"yelp.json\",\n",
    "}\n",
    "\n",
    "seed_count = 10  # How many different seeds do we expect?\n",
    "\n",
    "COMET_WORKSPACE = \"outlier-detection\"\n",
    "filter_names = [\n",
    "    \"HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS\"\n",
    "]\n",
    "# filter_names = [\"HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c8726a3f36ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:29.916779600Z",
     "start_time": "2024-02-21T11:54:29.886500800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_used_metrics(experiment_metrics: List[Dict[str, Any]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract a list of unique metrics from the given experiment metrics that are also present in the `metrics` list.\n",
    "\n",
    "    Args:\n",
    "        experiment_metrics (List[Dict[str, Any]]): A list of dictionaries where each dictionary\n",
    "            represents a metric with various attributes, including \"metricName\".\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of unique metric dictionaries where \"metricName\" exists in the global `metrics` list.\n",
    "    \"\"\"\n",
    "    metrics_used: List[Dict] = []\n",
    "    for metric in experiment_metrics:\n",
    "        if metric[\"metricName\"] in metrics:\n",
    "            metrics_used.append(metric)\n",
    "    return metrics_used\n",
    "\n",
    "\n",
    "def extract_paremeter_value(\n",
    "    parameters_used: List[Dict[str, Any]], parameter_name: str\n",
    ") -> str | float:\n",
    "    \"\"\"\n",
    "    Extracts the current value of a specified parameter from a list of parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters_used (List[Dict[str, Any]]): A list of dictionaries containing parameter information.\n",
    "        parameter_name (str): The name of the parameter to extract.\n",
    "\n",
    "    Returns:\n",
    "        str | float: The current value of the specified parameter.\n",
    "    \"\"\"\n",
    "    parameters_dict = [\n",
    "        entry for entry in parameters_used if entry.get(\"name\") == parameter_name\n",
    "    ]\n",
    "    return parameters_dict[0][\"valueCurrent\"]\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    experiment: comet.APIExperiment,\n",
    ") -> DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Loads and organizes experiment data, including metrics, parameters, and assets.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing experiment data.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]: A nested dictionary with tasks, seeds, and experiment data.\n",
    "    \"\"\"\n",
    "    data = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "    experiment_parameters = experiment.get_parameters_summary()\n",
    "    task = extract_paremeter_value(experiment_parameters, \"task\")\n",
    "    seed = extract_paremeter_value(experiment_parameters, \"seed\")\n",
    "\n",
    "    metrics_dict = {}\n",
    "    experiment_metrics = experiment.get_metrics()\n",
    "    metrics_used = extract_used_metrics(experiment_metrics)\n",
    "    for metric in metrics_used:\n",
    "        metric_name = metric[\"metricName\"]\n",
    "        metric_value = metric[\"metricValue\"]\n",
    "        if not metric_value:\n",
    "            continue\n",
    "\n",
    "        if metric_name not in metrics_dict:\n",
    "            metrics_dict[metric_name] = metric_value\n",
    "\n",
    "    params_dict = {}\n",
    "    for param_name in parameters.keys():\n",
    "        param_value = extract_paremeter_value(experiment_parameters, param_name)\n",
    "        if not param_value:\n",
    "            continue\n",
    "        else:\n",
    "            params_dict[param_name] = param_value\n",
    "\n",
    "    assets = download_assets(experiment, task, seed)\n",
    "\n",
    "    data[task][seed][\"metrics\"].update(metrics_dict)\n",
    "    data[task][seed][\"parameters\"].update(params_dict)\n",
    "    data[task][seed][\"assets\"].update(assets)\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_dataframe_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the data types of DataFrame columns based on predefined mappings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with converted column data types.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col in metrics:\n",
    "            df[col] = df[col].astype(metrics[col])\n",
    "        elif col in parameters:\n",
    "            df[col] = df[col].astype(parameters[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_data_frames(\n",
    "    experiment_data: List[\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]\n",
    "    ]\n",
    ") -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    \"\"\"\n",
    "    Creates DataFrames from experiment data and converts their data types.\n",
    "\n",
    "    Args:\n",
    "        experiment_data (List[DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]]): A list of nested dictionaries containing experiment data.\n",
    "\n",
    "    Returns:\n",
    "         DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]: A nested dictionary with pd.DataFrames for each section of the experiment data.\n",
    "    \"\"\"\n",
    "    for index, entry in enumerate(experiment_data):\n",
    "        for task, seed_dict in entry.items():\n",
    "            for seed, section_dict in seed_dict.items():\n",
    "                for section, hyperparameters_dict in section_dict.items():\n",
    "                    df = pd.DataFrame.from_dict([hyperparameters_dict])\n",
    "                    df_converted = convert_dataframe_types(df=df)\n",
    "\n",
    "                    entry[task][seed][section] = df_converted\n",
    "\n",
    "        experiment_data[index] = entry\n",
    "\n",
    "    return experiment_data\n",
    "\n",
    "\n",
    "def load_workspace_data(\n",
    "    project_name: str,\n",
    ") -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    \"\"\"\n",
    "    Loads experiment data for a specific project from the Comet workspace.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): The name of the project to load data from.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]: A dictionarty of nested dictionaries containing experiment data and DataFrames.\n",
    "    \"\"\"\n",
    "    experiments = API.get(workspace=COMET_WORKSPACE, project_name=project_name)\n",
    "    experiment_data = []\n",
    "    for exp in experiments:\n",
    "        try:\n",
    "            loaded_data = load_experiment_data(exp)\n",
    "            if loaded_data is not None:\n",
    "                experiment_data.append(loaded_data)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    experiment_data = create_data_frames(experiment_data=experiment_data)\n",
    "    return experiment_data\n",
    "\n",
    "\n",
    "def download_assets(experiment, task: str, seed: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Downloads and saves the assets of an experiment, filtering out unnecessary files.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing the experiment data.\n",
    "        task (str): The task name associated with the experiment.\n",
    "        seed (str): The seed value associated with the experiment.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: A dictionary of downloaded assets, loaded as NumPy arrays.\n",
    "    \"\"\"\n",
    "    assets = experiment.get_asset_list()\n",
    "    filtered_assets = [\n",
    "        asset\n",
    "        for asset in assets\n",
    "        if \"durations\" not in asset[\"fileName\"]\n",
    "        and not asset[\"fileName\"].endswith(\".py\")\n",
    "    ]\n",
    "\n",
    "    asset_ids = []\n",
    "    for asset in filtered_assets:\n",
    "        asset_ids.append((asset[\"fileName\"], asset[\"assetId\"]))\n",
    "\n",
    "    assets_downloaded = {}\n",
    "    for file_name, idx in asset_ids:\n",
    "        asset_data = experiment.get_asset(idx)\n",
    "        asset_path = Path(f\"./cache/assets/{task}/{seed}/{file_name}\")\n",
    "        asset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(asset_path, \"wb\") as f:\n",
    "            f.write(asset_data)\n",
    "        assets_downloaded[file_name[:-4]] = np.load(asset_path)\n",
    "    return assets_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e558f86ffdd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:31.285414Z",
     "start_time": "2024-02-21T11:54:31.252474400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_all_seeds(\n",
    "    data: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]],\n",
    "    task_name: str,\n",
    "):\n",
    "    collected_seeds = []\n",
    "\n",
    "    for data_dict in data:\n",
    "        seed = next(iter(data_dict[task_name]))\n",
    "        collected_seeds.append(seed)\n",
    "\n",
    "    return np.array(collected_seeds, dtype=int)\n",
    "\n",
    "\n",
    "def get_filter_strategy_name(\n",
    "    group: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]],\n",
    "    task_name: str,\n",
    "):\n",
    "    data = group[0]\n",
    "    seed = next(iter(data[task_name]))\n",
    "    filter_strategy = data[task_name][seed][\"parameters\"][\"filter_strategy_name\"].iat[0]\n",
    "\n",
    "    return filter_strategy\n",
    "\n",
    "\n",
    "def process_group(\n",
    "    group: List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]],\n",
    "    task_name: str,\n",
    "):\n",
    "\n",
    "    seeds = collect_all_seeds(group, task_name=task_name)\n",
    "    missing_seeds = np.setdiff1d(np.arange(42, 42 + seed_count), seeds)\n",
    "    print(len(missing_seeds))\n",
    "\n",
    "    # Take the first element of the list, look up the first seed (because every experiment has the same outlier filter strategy), and then take the result from the first row at column 'filter_strategy_name'\n",
    "    filter_strategy = get_filter_strategy_name(group, task_name)\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\"seed\": seed, \"filter\": filter_strategy, \"task\": task_name}\n",
    "            for seed in missing_seeds\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(workspace_name: str) -> pd.DataFrame:\n",
    "    loaded_data = load_workspace_data(workspace_name)\n",
    "    task = next(\n",
    "        iter(loaded_data[0])\n",
    "    )  # Get only back the first entry of the list, as all elements inside it are going to have the same task name\n",
    "    processed_data = process_group(loaded_data, task)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e300922",
   "metadata": {},
   "source": [
    "### (Optional) Find Missing Experimental Seeds\n",
    "\n",
    "This cell's purpose is to collect all the experimental runs that were designated towards a chosen `workspace` and ended up failing, i.e. due to sudden HPC issues that occured during execution in a CSV file called `missing_std_experiments.csv`. The following code sample from cells above shows where the `workspace` name gets assigned. Change it accordingly for your analysis.\n",
    "\n",
    "```py\n",
    "def load_workspace_data(project_name: str) -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    experiments = API.get(workspace=COMET_WORKSPACE, project_name=project_name)\n",
    "    ...\n",
    "```\n",
    "\n",
    "#### Structure of `missing_std_experiments.csv`\n",
    "\n",
    "In the following example the general structure of the generated file is displayed:\n",
    "| seed| filter | task |\n",
    "| --- | --- | --- |\n",
    "| 42 | HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS | rotten_tomatoes|\n",
    "\n",
    "- `seed`: Represents the seed that was used\n",
    "- `filter`: Is the name of the filter strategies that were used\n",
    "- `task`: The name of the workspace where it was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c0bc58138c54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T12:53:02.254155500Z",
     "start_time": "2024-02-21T11:54:32.912064200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collected_data: List[pd.DataFrame] = []\n",
    "for task in tqdm(task_names):\n",
    "    try:\n",
    "        data = load_data(task)\n",
    "        collected_data.append(data)\n",
    "    except:\n",
    "        missing_seeds = np.arange(42, 42 + seed_count)\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                {\"seed\": seed, \"filter\": filter_names[0], \"task\": task}\n",
    "                for seed in missing_seeds\n",
    "            ]\n",
    "        )\n",
    "        collected_data.append(df)\n",
    "        print(f\"{task}: Missing\")\n",
    "\n",
    "if collected_data:\n",
    "    missing_experiments = pd.concat(collected_data)\n",
    "    missing_experiments = missing_experiments[\n",
    "        missing_experiments[\"filter\"].isin(filter_names)\n",
    "    ]\n",
    "    missing_experiments.to_csv(\"missing_std_experiments.csv\")\n",
    "    missing_experiments  # 941, 856, 848, 762, 747, (856, 715, 650, 607, 516, 503, 384, 370, 322, 290, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ed05b234aadca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T14:30:05.879416900Z",
     "start_time": "2024-02-01T14:30:04.230197800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "filter_names_clean = {\n",
    "    \"LoserFilter_Plain\": \"Simple DSM\",\n",
    "    \"AutoFilter_Chen_Like\": \"Semantic AE\",\n",
    "    \"SingleStepEntropy_SimplePseudo\": \"Simple SS\",\n",
    "    \"HDBScanFilter\": \"HDBScan\",\n",
    "    \"IsolationForestFilter\": \"IsolationForest\",\n",
    "    \"LocalOutlierFactorFilter\": \"LocalOutlierFactor\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_available_workspace_data() -> (\n",
    "    List[List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves the workspace data for a list of tasks.\n",
    "\n",
    "    This function iterates over the available task names, loads their respective\n",
    "    workspace data, and appends the data to a list.\n",
    "\n",
    "    Returns:\n",
    "        List[List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]]:\n",
    "            A nested list where each element contains the experimental data for\n",
    "            different tasks, represented as a defaultdict structure with task names,\n",
    "            seeds, hyperparameters, and pandas DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    workspace_data = []\n",
    "    for task_name in task_names:\n",
    "        data = load_workspace_data(task_name)\n",
    "        workspace_data.append(data)\n",
    "\n",
    "    return workspace_data\n",
    "\n",
    "\n",
    "def extract_asset_data(\n",
    "    workspace_data: List[\n",
    "        List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]\n",
    "    ]\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Extracts the 'assets' DataFrame from the workspace data for each task and seed.\n",
    "\n",
    "    This function processes the nested workspace data and extracts the 'assets'\n",
    "    DataFrame, ensuring that duplicate seeds are not included. It stores the 'assets'\n",
    "    DataFrame in a dictionary with task names and seeds as keys.\n",
    "\n",
    "    Args:\n",
    "        workspace_data (List[List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]]):\n",
    "            The nested list structure containing experimental data for different\n",
    "            tasks, seeds, and hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, pd.DataFrame]]: A dictionary where the key is the task\n",
    "        name and the value is another dictionary with seeds as keys and the corresponding\n",
    "        'assets' DataFrame as values.\n",
    "    \"\"\"\n",
    "    asset_data = {}\n",
    "    for experiment in workspace_data:\n",
    "        for experimental_task in experiment:\n",
    "            for task_name, seed_dict in experimental_task.items():\n",
    "                for seed, hyperparameter_dict in seed_dict.items():\n",
    "                    assets_df = hyperparameter_dict.get(\"assets\", None)\n",
    "                    if not assets_df.empty:\n",
    "                        if task_name not in asset_data:\n",
    "                            asset_data[task_name] = {}\n",
    "\n",
    "                        if seed not in asset_data[task_name]:\n",
    "                            asset_data[task_name][seed] = assets_df\n",
    "\n",
    "    return asset_data\n",
    "\n",
    "\n",
    "def clean_up_asset_name(asset_name: str) -> str:\n",
    "    \"\"\"This function removes the `_no_htl` and `_random` extension from the asset.\n",
    "    For example: `AutoFilter_Chen_Like_no_htl` would be transformed into `AutoFilter_Chen_Like`.\n",
    "    Args:\n",
    "        asset_name (str): The name of the asset to be (potentially) changed, as some do not have the aforementioned suffixes\n",
    "\n",
    "    Returns:\n",
    "        str: The updated asset_name\n",
    "    \"\"\"\n",
    "    if asset_name.endswith(\"_no_htl\") or asset_name.endswith(\"_random\"):\n",
    "        asset_name = asset_name[:-7]\n",
    "\n",
    "    cleaned_asset = filter_names_clean[asset_name]\n",
    "    return cleaned_asset\n",
    "\n",
    "\n",
    "def calculate_averages(\n",
    "    asset_data: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> DefaultDict[str, DefaultDict[str, np.float64]]:\n",
    "    \"\"\"\n",
    "    Calculates the average of the 'assets' DataFrames across different seeds for each task.\n",
    "\n",
    "    This function concatenates the 'assets' DataFrames for each task, computes the\n",
    "    element-wise mean (handling np.array values), and then calculates the overall\n",
    "    mean across all rows.\n",
    "\n",
    "    Args:\n",
    "        asset_data (Dict[str, Dict[str, pd.DataFrame]]):\n",
    "            A dictionary where the key is the task name and the value is another\n",
    "            dictionary with seeds as keys and the corresponding 'assets' DataFrame as values.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: A dictionary where the key is the task name and the\n",
    "        value is a DataFrame containing the average values across the seeds for that task.\n",
    "    \"\"\"\n",
    "    summarised_data = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    for task_name, seed_dict in asset_data.items():\n",
    "\n",
    "        collected_dfs = list(seed_dict.values())\n",
    "        merged_df = pd.concat(collected_dfs, ignore_index=True)\n",
    "        for asset_name in merged_df.columns:\n",
    "            arrays_for_col = merged_df[asset_name].values\n",
    "            flattened_arrays = np.concatenate(arrays_for_col)\n",
    "            mean = np.mean(flattened_arrays)\n",
    "            summarised_data[task_name][asset_name] = mean\n",
    "\n",
    "    return summarised_data\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    workspace_data = get_available_workspace_data()\n",
    "    asset_data = extract_asset_data(workspace_data=workspace_data)\n",
    "    summarised_data = calculate_averages(asset_data)\n",
    "    return summarised_data\n",
    "\n",
    "def transform_into_percentual_difference(asset_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in asset_df.columns:\n",
    "        if col != \"HTL\":\n",
    "            asset_df[col] = (asset_df[col] - asset_df[\"HTL\"]).mul(100)\n",
    "\n",
    "    return asset_df\n",
    "\n",
    "\n",
    "def filter_no_htl(asset_name: str) -> bool:\n",
    "    return asset_name.endswith(\"_no_htl\")\n",
    "\n",
    "\n",
    "def filter_random(asset_name: str) -> bool:\n",
    "    return asset_name.endswith(\"_random\")\n",
    "\n",
    "\n",
    "def create_comparison_df(\n",
    "    data: DefaultDict[str, DefaultDict[str, np.float64]],\n",
    "    filter_condition: Callable[[str], bool]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    dfs: list[pd.DataFrame] = []\n",
    "    for task_name, asset_dict in data.items():\n",
    "        collected_assets = {}\n",
    "        for asset_name, asset_value in asset_dict.items():\n",
    "            if filter_condition(asset_name):\n",
    "                cleaned_asset = clean_up_asset_name(asset_name)\n",
    "                collected_assets[cleaned_asset] = asset_value\n",
    "            elif asset_name.endswith(\"HTL\"):\n",
    "                collected_assets[asset_name] = asset_value\n",
    "\n",
    "        asset_df = transform_into_percentual_difference(asset_df=pd.DataFrame(data=collected_assets, index=[task_name]))\n",
    "        dfs.append(asset_df)\n",
    "\n",
    "    merged_df = pd.concat(dfs)\n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "    summarised_data = prepare_data()\n",
    "    data_filtered = create_comparison_df(summarised_data, filter_no_htl)\n",
    "    data_unfiltered = create_comparison_df(summarised_data, filter_random)\n",
    "\n",
    "    sns.set_theme()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.heatmap(data=data_filtered[data_filtered.columns.difference([\"HTL\"])], annot=True, ax=axes[0])\n",
    "    axes[0].set_title('Filtered vs Unfiltered')\n",
    "    sns.heatmap(data=data_unfiltered[data_unfiltered.columns.difference([\"HTL\"])], annot=True, ax= axes[1])\n",
    "    axes[1].set_title('Random(Filled Up) vs Unfiltered')\n",
    "\n",
    "\n",
    "main()\n",
    "# def add_boxplots(results, filter_strategy, l:list):\n",
    "#     # Adds BoxPlot to the graph\n",
    "#     # Adds multiple medians to the graph\n",
    "#     for strategy in filter_strategy:\n",
    "#         data = []\n",
    "#         for task in task_names:\n",
    "#             data += list(results[task][\"f1s\"][strategy])\n",
    "#         l.append(data)\n",
    "\n",
    "# l = []\n",
    "# filter_names_ = [\n",
    "#     \"AutoFilter_Chen_Like\",\n",
    "#     \"LoserFilter_Plain\",\n",
    "#     \"SingleStepEntropy_SimplePseudo\",\n",
    "#     \"HDBScanFilter\",\n",
    "#     \"IsolationForestFilter\",\n",
    "#     \"LocalOutlierFactorFilter\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# add_boxplots(rotten_tomatoes, filter, l)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# intra_group_dist = 0.75\n",
    "# inter_group_dist = 1.5\n",
    "# positions = [i * intra_group_dist + (i // 3) * inter_group_dist for i in range(len(l))]\n",
    "# bp = ax.boxplot(l, showfliers=False, positions=positions, patch_artist=True)\n",
    "# colors = ['#FF7F50', '#7cda9e', '#8fdeff']\n",
    "# for i, patch in enumerate(bp['boxes']):\n",
    "#     patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "\n",
    "# for i, median in enumerate(bp['medians']):\n",
    "#     median_x, median_y = median.get_xydata()[1]  # Get the median line's X and Y data\n",
    "#     # Hide the median line\n",
    "#     median.set_visible(False)\n",
    "#     # Plot a diamond marker at the median position\n",
    "#     offset = 0.25\n",
    "#     ax.plot(median_x-offset, median_y, 'd', color='#082239', markersize=3)\n",
    "\n",
    "# tick_positions = [np.mean(positions[(i*3):(i*3)+3])-len(filter_names_clean[f])*0.18 for i, f in enumerate(filter_names_)]\n",
    "\n",
    "# plt.xticks(tick_positions, [filter_names_clean[f] for f in filter_names_], rotation=45)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"endresults.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea530926c4efd2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T14:44:03.295575500Z",
     "start_time": "2024-02-01T14:43:55.053909700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final Performance Graphs\n",
    "\n",
    "for task in task_names:\n",
    "\n",
    "    def add_boxplots(results, filter, l: list):\n",
    "        # Adds BoxPlot to the graph\n",
    "        # Adds multiple medians to the graph\n",
    "        for f in [\"NoneR\", filter, \"NoneE\"]:\n",
    "            data = []\n",
    "            data += list(results[task][\"f1s\"][f])\n",
    "            l.append(data)\n",
    "\n",
    "    l = []\n",
    "    filter_names_ = [\n",
    "        \"AutoFilter_LSTM_SIMPLE\",\n",
    "        \"AutoFilter_LSTM\",\n",
    "        \"AutoFilter_Chen_Like\",\n",
    "        \"LoserFilter_Plain\",\n",
    "        \"LoserFilter_Optimized_Pseudo_Labels\",\n",
    "        \"LoserFilter_SSL_Variety\",\n",
    "        \"TeachingFilter\",\n",
    "        \"TeachingFilter_WOW\",\n",
    "        \"TeachingFilter_Smooth\",\n",
    "        \"SingleStepEntropy_SimplePseudo\",\n",
    "        \"SingleStepEntropy\",\n",
    "    ]\n",
    "\n",
    "    filter_names_clean = {\n",
    "        \"LoserFilter_SSL_Variety\": \"EXPANDED DSM\",\n",
    "        \"LoserFilter_Plain\": \"SIMPLE DSM\",\n",
    "        \"LoserFilter_Optimized_Pseudo_Labels\": \"MC DSM\",\n",
    "        \"AutoFilter_Chen_Like\": \"SEMANTIC AE\",\n",
    "        \"AutoFilter_LSTM\": \"LSTM ENSEMBLE AE\",\n",
    "        \"AutoFilter_LSTM_SIMPLE\": \"SIMPLE LSTM AE\",\n",
    "        \"SingleStepEntropy\": \"MC SS\",\n",
    "        \"SingleStepEntropy_SimplePseudo\": \"SIMPLE SS\",\n",
    "        \"TeachingFilter\": \"SIMPLE LE\",\n",
    "        \"TeachingFilter_Smooth\": \"SMOOTH LE\",\n",
    "        \"TeachingFilter_WOW\": \"HIGH ENTROPY LE\",\n",
    "    }\n",
    "\n",
    "    for filter in filter_names_:\n",
    "        add_boxplots(results, filter, l)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    intra_group_dist = 0.75\n",
    "    inter_group_dist = 1.5\n",
    "    positions = [\n",
    "        i * intra_group_dist + (i // 3) * inter_group_dist for i in range(len(l))\n",
    "    ]\n",
    "    bp = ax.boxplot(l, showfliers=False, positions=positions, patch_artist=True)\n",
    "    colors = [\"#FF7F50\", \"#7cda9e\", \"#8fdeff\"]\n",
    "    for i, patch in enumerate(bp[\"boxes\"]):\n",
    "        patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "    for i, median in enumerate(bp[\"medians\"]):\n",
    "        median_x, median_y = median.get_xydata()[\n",
    "            1\n",
    "        ]  # Get the median line's X and Y data\n",
    "        # Hide the median line\n",
    "        median.set_visible(False)\n",
    "        # Plot a diamond marker at the median position\n",
    "        offset = 0.25\n",
    "        ax.plot(median_x - offset, median_y, \"d\", color=\"#082239\", markersize=3)\n",
    "\n",
    "    tick_positions = [\n",
    "        np.mean(positions[(i * 3) : (i * 3) + 3]) - len(filter_names_clean[f]) * 0.18\n",
    "        for i, f in enumerate(filter_names_)\n",
    "    ]\n",
    "\n",
    "    plt.xticks(\n",
    "        tick_positions, [filter_names_clean[f] for f in filter_names_], rotation=45\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"endresults-{task}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03d3ef80819d8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ChatGPT Example for Above, can remove this cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the figure and axes for the boxplots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Incremental Boxplots\")\n",
    "\n",
    "# This list will hold different datasets for individual boxplots\n",
    "datasets = []\n",
    "\n",
    "# Assuming you have a mechanism to add datasets one at a time\n",
    "for i in range(1, 6):  # Example loop to simulate adding 5 datasets incrementally\n",
    "    # Simulate generating or loading a new dataset\n",
    "    new_data = np.random.normal(loc=i, scale=0.5, size=100)\n",
    "    datasets.append(new_data)  # Add the new dataset to the list\n",
    "\n",
    "# Clear the axes for fresh plot (optional if you want to redraw the boxplots)\n",
    "ax.clear()\n",
    "# Plot all the current datasets as individual boxplots\n",
    "ax.boxplot(datasets)\n",
    "ax.set_title(\"Incremental Boxplots\")\n",
    "plt.draw()  # Redraw the plot with the new data\n",
    "plt.pause(0.5)  # Pause to visually confirm the addition, adjust or remove as needed\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9d191b06d81d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5.3 Significance Test from Paper\n",
    "\n",
    "# HTL mit NO HTL Vergleich & Random mit NO HTL vergleichen\n",
    "import deepsig\n",
    "import pandas as pd\n",
    "\n",
    "aso_test = {}\n",
    "for filter_name in filter_names:\n",
    "    data = []\n",
    "    task_aso = {}\n",
    "    for task in task_names:\n",
    "        htl = results[task][\"f1s\"][\"NoneR\"]\n",
    "        no_htl = results[task][\"f1s\"][filter_name]\n",
    "        better = deepsig.aso(no_htl, htl, seed=42)\n",
    "        task_aso[task + \"_no_htl_is_better\"] = better\n",
    "    aso_test[filter_name] = task_aso\n",
    "\n",
    "pd.DataFrame(aso_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
