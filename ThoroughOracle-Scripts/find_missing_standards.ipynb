{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:53:16.948257700Z",
     "start_time": "2024-02-21T11:53:15.571641600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import comet_ml as comet\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Any, DefaultDict, Optional\n",
    "from collections.abc import Callable\n",
    "import numpy as np\n",
    "import textwrap\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "API = comet.API()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa7d31",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "**NOTE:** It is expected that the Notebooks are run **inside VS Code** as it allows the pathing for `task_configs` to work. If it is run outside a VS Code instance, please adjust the following line:\n",
    "\n",
    "```py\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "```\n",
    "\n",
    "The cell down below has the following configuration attributes, which might need adjustment depending on changes of the experimental design\n",
    "\n",
    "- `metrics`: Inside this dictionary the keys represent the actual names of the metric, as they are displayed on `comet`, while the values are simply just given the according type that will be fetched from online.\n",
    "  \n",
    "- `parameters`: The parameters describe general experimental setup information, which were passed as arguments upon execution\n",
    "  \n",
    "- `task_names`: The task names represent the data sets upon which the Outlier Detection Strategies were trained on\n",
    "  \n",
    "- `task_configs`: The task configs represent the path to the configuration files of the `task names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072bf8989680c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:28.203914300Z",
     "start_time": "2024-02-21T11:54:28.153936800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"AutoFilter_Chen_Like_HTL Count\": float,\n",
    "    \"AutoFilter_Chen_Like_avg_duration\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (random replacement)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (No HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_avgF1 (With HTL)\": float,\n",
    "    \"AutoFilter_Chen_Like_medF1 (random replacement)\": float,\n",
    "    \"HDBScanFilter_HTL Count\": float,\n",
    "    \"HDBScanFilter_avg_duration\": float,\n",
    "    \"HDBScanFilter_medF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_medF1 (random replacement)\": float,\n",
    "    \"HDBScanFilter_avgF1 (No HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (With HTL)\": float,\n",
    "    \"HDBScanFilter_avgF1 (random replacement)\": float,\n",
    "    \"IsolationForestFilter_HTL Count\": float,\n",
    "    \"IsolationForestFilter_avg_duration\": float,\n",
    "    \"IsolationForestFilter_avgF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_avgF1 (random replacement)\": float,\n",
    "    \"IsolationForestFilter_medF1 (No HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (With HTL)\": float,\n",
    "    \"IsolationForestFilter_medF1 (random replacement)\": float,\n",
    "    \"LocalOutlierFactorFilter_HTL Count\": float,\n",
    "    \"LocalOutlierFactorFilter_avg_duration\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_avgF1 (random replacement)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (No HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (With HTL)\": float,\n",
    "    \"LocalOutlierFactorFilter_medF1 (random replacement)\": float,\n",
    "    \"LoserFilter_Plain_HTL Count\": float,\n",
    "    \"LoserFilter_Plain_avg_duration\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_avgF1 (random replacement)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (No HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (With HTL)\": float,\n",
    "    \"LoserFilter_Plain_medF1 (random replacement)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_HTL Count\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avg_duration\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_avgF1 (random replacement)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (No HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (With HTL)\": float,\n",
    "    \"SingleStepEntropy_SimplePseudo_medF1 (random replacement)\": float,\n",
    "}\n",
    "parameters = {\n",
    "    \"strategy_name\": str,\n",
    "    \"filter_strategy_name\": str,\n",
    "    \"seed\": int,\n",
    "    \"task\": str,\n",
    "}\n",
    "\n",
    "task_names = [\n",
    "    \"ag-news\",\n",
    "    \"dbpedia\",\n",
    "    \"fnc1\",\n",
    "    \"imdb\",\n",
    "    \"qnli\",\n",
    "    \"rotten-tomatoes\",\n",
    "    \"sst2\",\n",
    "    \"trec\",\n",
    "    \"wiki-talk\",\n",
    "]\n",
    "\n",
    "version = \"x\"\n",
    "task_names = [version + t for t in task_names]\n",
    "\n",
    "# This gets the location of the Notebook, needs VSCode to be executed correctly\n",
    "notebook_name = \"/\".join(\n",
    "    IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[-5:]\n",
    ")\n",
    "\n",
    "BASE_PATH = Path(notebook_name).parent\n",
    "CONFIGS_PATH = BASE_PATH.parent / \"Configs\" / \"Tasks\"\n",
    "\n",
    "task_configs = {\n",
    "    \"xAG News\": CONFIGS_PATH / \"ag_news.json\",\n",
    "    \"xDBPedia\": CONFIGS_PATH / \"dbpedia.json\",\n",
    "    \"xFNC1\": CONFIGS_PATH / \"fnc_one.json\",\n",
    "    \"xIMDB\": CONFIGS_PATH / \"imdb.json\",\n",
    "    \"xQNLI\": CONFIGS_PATH / \"qnli.json\",\n",
    "    \"xRotten Tomatoes\": CONFIGS_PATH / \"rotten_tomatoes.json\",\n",
    "    \"xSST2\": CONFIGS_PATH / \"sst2.json\",\n",
    "    \"xTREC\": CONFIGS_PATH / \"trec.json\",\n",
    "    \"xWiki Talk\": CONFIGS_PATH / \"wiki_talk.json\",\n",
    "}\n",
    "\n",
    "seed_count = 20  # How many different seeds do we expect?\n",
    "\n",
    "COMET_WORKSPACE = \"sklearn-20\"\n",
    "filter_names = [\n",
    "    \"HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c8726a3f36ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:54:29.916779600Z",
     "start_time": "2024-02-21T11:54:29.886500800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_used_metrics(experiment_metrics: List[Dict[str, Any]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract a list of unique metrics from the given experiment metrics that are also present in the `metrics` list.\n",
    "\n",
    "    Args:\n",
    "        experiment_metrics (List[Dict[str, Any]]): A list of dictionaries where each dictionary\n",
    "            represents a metric with various attributes, including \"metricName\".\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of unique metric dictionaries where \"metricName\" exists in the global `metrics` list.\n",
    "    \"\"\"\n",
    "    metrics_used: List[Dict] = []\n",
    "    for metric in experiment_metrics:\n",
    "        if metric[\"metricName\"] in metrics:\n",
    "            metrics_used.append(metric)\n",
    "    return metrics_used\n",
    "\n",
    "\n",
    "def extract_paremeter_value(\n",
    "    parameters_used: List[Dict[str, Any]], parameter_name: str\n",
    ") -> str | float:\n",
    "    \"\"\"\n",
    "    Extracts the current value of a specified parameter from a list of parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters_used (List[Dict[str, Any]]): A list of dictionaries containing parameter information.\n",
    "        parameter_name (str): The name of the parameter to extract.\n",
    "\n",
    "    Returns:\n",
    "        str | float: The current value of the specified parameter.\n",
    "    \"\"\"\n",
    "    parameters_dict = [\n",
    "        entry for entry in parameters_used if entry.get(\"name\") == parameter_name\n",
    "    ]\n",
    "    return parameters_dict[0][\"valueCurrent\"]\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    experiment: comet.APIExperiment,\n",
    ") -> DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Loads and organizes experiment data, including metrics, parameters, and assets.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing experiment data.\n",
    "\n",
    "    Returns:\n",
    "        DefaultDict[str, DefaultDict[str, DefaultDict[str, Dict[str, Any]]]]: A nested dictionary with tasks, seeds, and experiment data.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_parameters = experiment.get_parameters_summary()\n",
    "    task = extract_paremeter_value(experiment_parameters, \"task\")\n",
    "    seed = extract_paremeter_value(experiment_parameters, \"seed\")\n",
    "\n",
    "    download_assets(experiment, task, seed)\n",
    "\n",
    "\n",
    "\n",
    "def download_assets(experiment:comet.APIExperiment, task: str, seed: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Downloads and saves the assets of an experiment, filtering out unnecessary files.\n",
    "\n",
    "    Args:\n",
    "        experiment (comet.APIExperiment): A Comet APIExperiment object containing the experiment data.\n",
    "        task (str): The task name associated with the experiment.\n",
    "        seed (str): The seed value associated with the experiment.\n",
    "        project_name (str): The name of the project associated with the experiment.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: A dictionary of downloaded assets, loaded as NumPy arrays.\n",
    "    \"\"\"\n",
    "    assets = experiment.get_asset_list()\n",
    "    filtered_assets = [\n",
    "        asset\n",
    "        for asset in assets\n",
    "        if \"durations\" not in asset[\"fileName\"]\n",
    "        and not asset[\"fileName\"].endswith(\".py\")\n",
    "    ]\n",
    "\n",
    "    asset_ids = []\n",
    "    for asset in filtered_assets:\n",
    "        asset_ids.append((asset[\"fileName\"], asset[\"assetId\"]))\n",
    "\n",
    "    for file_name, idx in asset_ids:\n",
    "        asset_data = experiment.get_asset(idx)\n",
    "        asset_path = Path(f\"./cache/assets/{COMET_WORKSPACE}/{task}/{seed}/{file_name}\")\n",
    "        asset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(asset_path, \"wb\") as f:\n",
    "            f.write(asset_data)\n",
    "\n",
    "\n",
    "def download_workspace_data(task_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Loads experiment data for a specific project from the Comet workspace.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): The name of the project to load data from.\n",
    "    \"\"\"\n",
    "    experiments = API.get(workspace=COMET_WORKSPACE, project_name=task_name)\n",
    "    for exp in experiments:\n",
    "        load_experiment_data(exp)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    for task_name in task_names:\n",
    "        download_workspace_data(task_name)\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer_directory(dir_path: Path) -> bool:\n",
    "    \"\"\"Check if the directory name is an integer.\"\"\"\n",
    "    return dir_path.name.isdigit()\n",
    "\n",
    "def collect_asset_paths(ASSET_PATHS: Optional[Path] = Path(\"cache\", \"assets\", COMET_WORKSPACE)) -> DefaultDict[str, List]:\n",
    "    TASK_ASSET_MAP = defaultdict(list)\n",
    "    for dir_path in ASSET_PATHS.glob('**/*'):\n",
    "        if dir_path.is_dir() and is_integer_directory(dir_path):\n",
    "            task_name = dir_path.parent.name\n",
    "            if task_name != COMET_WORKSPACE:\n",
    "                TASK_ASSET_MAP[task_name].append(dir_path)\n",
    "\n",
    "    return TASK_ASSET_MAP\n",
    "\n",
    "def find_missing_seeds_count(task_asset_map):\n",
    "    seed_map = defaultdict(list)\n",
    "    for task, asset_paths in task_asset_map.items():\n",
    "        seeds = [int(path.name) for path in asset_paths]\n",
    "        missing_seeds: List | None = np.setdiff1d(np.arange(42, 42 + seed_count), seeds).tolist()\n",
    "        seed_map[task] = missing_seeds\n",
    "\n",
    "\n",
    "    return seed_map\n",
    "\n",
    "\n",
    "TASK_ASSET_MAP = collect_asset_paths()\n",
    "missing_seeds = find_missing_seeds_count(task_asset_map=TASK_ASSET_MAP)\n",
    "missing_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e300922",
   "metadata": {},
   "source": [
    "### (Optional) Find Missing Experimental Seeds\n",
    "\n",
    "This cell's purpose is to collect all the experimental runs that were designated towards a chosen `workspace` and ended up failing, i.e. due to sudden HPC issues that occured during execution in a CSV file called `missing_std_experiments.csv`. The following code sample from cells above shows where the `workspace` name gets assigned. Change it accordingly for your analysis.\n",
    "\n",
    "```py\n",
    "def load_workspace_data(project_name: str) -> List[DefaultDict[str, DefaultDict[str, DefaultDict[str, pd.DataFrame]]]]:\n",
    "    experiments = API.get(workspace=COMET_WORKSPACE, project_name=project_name)\n",
    "    ...\n",
    "```\n",
    "\n",
    "#### Structure of `missing_std_experiments.csv`\n",
    "\n",
    "In the following example the general structure of the generated file is displayed:\n",
    "| seed| filter | task |\n",
    "| --- | --- | --- |\n",
    "| 42 | HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS | rotten_tomatoes|\n",
    "\n",
    "- `seed`: Represents the seed that was used\n",
    "- `filter`: Is the name of the filter strategies that were used\n",
    "- `task`: The name of the workspace where it was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c0bc58138c54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T12:53:02.254155500Z",
     "start_time": "2024-02-21T11:54:32.912064200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collected_data: List[pd.DataFrame] = []\n",
    "\n",
    "def get_task_config(task: Path) -> Path:\n",
    "    \"\"\"Transform in the input `task` into a Path object.\n",
    "\n",
    "    Args:\n",
    "        task (Path): A Path object, containing the full Path to the config\n",
    "\n",
    "    Returns:\n",
    "        Path: A relative Path based on the `task_configs` dict based on the relative Path: `Configs/Task/`\n",
    "    \"\"\"\n",
    "    return task.relative_to(task.parents[2])\n",
    "\n",
    "\n",
    "for task_name, missed_seeds in missing_seeds.items():\n",
    "    task = get_task_config(task_configs[task_name])\n",
    "    if missed_seeds:\n",
    "        df = pd.DataFrame(\n",
    "        [\n",
    "            {\"seed\": seed, \"filter\": filter_names[0], \"task\": task, \"workspace\": COMET_WORKSPACE}\n",
    "            for seed in missed_seeds\n",
    "        ]\n",
    "        )\n",
    "        collected_data.append(df)\n",
    "\n",
    "if collected_data:\n",
    "    missing_experiments = pd.concat(collected_data)\n",
    "    missing_experiments = missing_experiments[\n",
    "        missing_experiments[\"filter\"].isin(filter_names)\n",
    "    ]\n",
    "    missing_experiments.to_csv(\"missing_std_experiments.csv\", mode=\"a\")\n",
    "    missing_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intervals(seeds: list[int]) -> List[List[int]]:\n",
    "    \"\"\"Finds the largest contiguous intervals in a list of integers. It groups consecutive integers into a sublist.\n",
    "\n",
    "    Args:\n",
    "        seeds (list[int]): A list of integers to find (all and) the largest intervals from.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provided list for `seeds` is empty.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: A list of lists, where each inner list contains a contiguous interval of integers.\n",
    "\n",
    "    Examples:\n",
    "        >>> find_intervals([1, 2, 3, 5, 6, 7, 9])\n",
    "        [[1, 2, 3], [5, 6, 7], [9]]\n",
    "\n",
    "        >>> find_intervals([10, 11, 13, 14, 15, 16])\n",
    "        [[10, 11], [13, 14, 15, 16]]\n",
    "    \"\"\"\n",
    "    if not seeds:\n",
    "        raise ValueError(\n",
    "            f\"The provided list for `seeds` ({seeds}) has: {len(seeds)} elements!\"\n",
    "        )\n",
    "    intervals = []\n",
    "\n",
    "    sub_interval = [seeds[0]]\n",
    "\n",
    "    for index in range(1, len(seeds)):\n",
    "        if seeds[index] == seeds[index - 1] + 1:\n",
    "            sub_interval.append(seeds[index])\n",
    "        else:\n",
    "            # Add the completed interval to the result and start a new interval right after\n",
    "            intervals.append(sub_interval)\n",
    "            sub_interval = [seeds[index]]\n",
    "\n",
    "    # Append the last interval, as it only gets added within the iteration, so the last one would be missing\n",
    "    intervals.append(sub_interval)\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def generate_task_scripts(\n",
    "    intervals: List[List[int]], task_config: str, workspace: str\n",
    ") -> None:\n",
    "    for index, interval in enumerate(intervals):\n",
    "        start = interval[0] - 42\n",
    "        end = interval[-1] - 42\n",
    "\n",
    "        # A task has the structure \"Configs/Tasks/<task_name>.json\" -> we want the <task_name>.json, then remove the \".json\"\n",
    "        task_name_cleaned = task_config.split(\"/\")[-1].split(\".\")[0]\n",
    "        # Common SLURM script header\n",
    "        slurm_header = textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            #!/bin/bash\n",
    "            #SBATCH --nodes=1              # request 1 node\n",
    "            #SBATCH --cpus-per-task=6      # use 6 threads per task\n",
    "            #SBATCH --gres=gpu:1           # use 1 GPU per node (i.e. use one GPU per task)\n",
    "            #SBATCH --time=100:00:00       # run for 100 hours\n",
    "            #SBATCH --mem=10G\n",
    "            #SBATCH --account=p_ml_il\n",
    "            #SBATCH --job-name={workspace}-{task_config}\n",
    "            #SBATCH --output=./slurm-runs/{workspace}/{task_name_cleaned}-%a-%A-%j.out\n",
    "            #SBATCH --exclude=i8008,i8009,i8011,i8014,i8021,i8023\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Array setup if start and end differ\n",
    "        array_directive = f\"#SBATCH --array={start}-{end}\\n\" if start != end else \"\"\n",
    "\n",
    "        # Command to calculate random seed\n",
    "        seed_command = (\n",
    "            f\"random_seed=$((42 + ${{SLURM_ARRAY_TASK_ID}}))\"\n",
    "            if start != end\n",
    "            else f\"random_seed={interval[0]}\"\n",
    "        )\n",
    "\n",
    "        # Common SLURM script body\n",
    "        slurm_body = textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            {array_directive}\n",
    "            module --force purge\n",
    "            module load release/23.04 GCC/11.3.0 Python/3.10.4\n",
    "            source /data/horse/ws/toma076c-outlier-detection/venv/bin/activate\n",
    "\n",
    "            nvidia-smi\n",
    "            hostname\n",
    "\n",
    "            # Calculate the random seed within the SLURM script\n",
    "            {seed_command}\n",
    "\n",
    "            echo \"Seed: ${{random_seed}}\"\n",
    "\n",
    "            srun python3 main.py \\\n",
    "                --task_config ./{task_config} \\\n",
    "                --experiment_config ./Configs/standard.json \\\n",
    "                --filter_strategy_name HDBScanFilter LocalOutlierFactorFilter IsolationForestFilter SimpleDSM SemanticAE SimpleSS \\\n",
    "                --comet_api_key {API.api_key} \\\n",
    "                --comet_workspace {workspace} \\\n",
    "                --random_seed ${{random_seed}}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Write the complete SLURM script\n",
    "        with open(f\"{workspace}-{task_name_cleaned}-{index}.sh\", \"w+\") as rsh:\n",
    "            rsh.write(slurm_header + slurm_body)\n",
    "\n",
    "\n",
    "def collect_missing_experiment_seeds(missing_experiments: pd.DataFrame) -> None:\n",
    "    unique_tasks = missing_experiments[\"task\"].unique()\n",
    "    for task in unique_tasks:\n",
    "        # We only care for all the rows that match the current task, so we create a subset of the df\n",
    "        task_df = missing_experiments.loc[missing_experiments[\"task\"] == task]\n",
    "        seeds = sorted(\n",
    "            task_df[\"seed\"].values\n",
    "        )  # Sort the seeds, to assure the biggest intervals are found\n",
    "        workspace = task_df[\"workspace\"].iloc[0]\n",
    "        task_intervals = find_intervals(seeds)\n",
    "        generate_task_scripts(intervals=task_intervals, task_config=task, workspace=workspace)\n",
    "\n",
    "\n",
    "# missing_experiments = pd.read_csv(\"missing_std_experiments.csv\")\n",
    "# x = collect_missing_experiment_seeds(missing_experiments=missing_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd640506",
   "metadata": {},
   "source": [
    "### Plot Filter vs No Filter & Filter vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ed05b234aadca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T14:30:05.879416900Z",
     "start_time": "2024-02-01T14:30:04.230197800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_names_clean = {\n",
    "    \"LoserFilter_Plain\": \"Simple DSM\",\n",
    "    \"AutoFilter_Chen_Like\": \"Semantic AE\",\n",
    "    \"SingleStepEntropy_SimplePseudo\": \"Simple SS\",\n",
    "    \"HDBScanFilter\": \"HDBScan\",\n",
    "    \"IsolationForestFilter\": \"IsolationForest\",\n",
    "    \"LocalOutlierFactorFilter\": \"LocalOutlierFactor\",\n",
    "}\n",
    "\n",
    "def load_asset_data(workspace_data: DefaultDict[str, List[Path]]) -> Dict[str, pd.DataFrame]:\n",
    "\n",
    "    asset_data = {}\n",
    "    for task_name, asset_dirs in workspace_data.items():\n",
    "        collected_assets = []\n",
    "        for asset_dir in asset_dirs:\n",
    "            asset_paths = asset_dir.glob(\"*\")\n",
    "            collected_dfs = []\n",
    "            for asset_path in asset_paths:\n",
    "                data = np.load(asset_path)\n",
    "                if data.size == 0:\n",
    "                    print(f\"{asset_path} is empty\")\n",
    "                df = pd.DataFrame(data={asset_path.name[:-4]: [data]})\n",
    "                if df.empty:\n",
    "                    print(f\"Careful! For '{task_name}', the '{asset_path}' file is empty.\")\n",
    "                else:\n",
    "                    collected_dfs.append(df)\n",
    "\n",
    "            collected_assets.append(pd.concat(collected_dfs, axis=1))\n",
    "\n",
    "        asset_data[task_name] = pd.concat(collected_assets, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    return asset_data\n",
    "\n",
    "\n",
    "def clean_up_asset_name(asset_name: str) -> str:\n",
    "    \"\"\"\n",
    "    This function removes the `_no_htl` and `_random` extension from the asset.\n",
    "    For example: `AutoFilter_Chen_Like_no_htl` would be transformed into `AutoFilter_Chen_Like`.\n",
    "    Args:\n",
    "        asset_name (str): The name of the asset to be (potentially) changed, as some do not have the aforementioned suffixes\n",
    "\n",
    "    Returns:\n",
    "        str: The updated asset_name\n",
    "    \"\"\"\n",
    "    if asset_name.endswith(\"_no_htl\") or asset_name.endswith(\"_random\"):\n",
    "        asset_name = asset_name[:-7]\n",
    "\n",
    "    cleaned_asset = filter_names_clean[asset_name]\n",
    "    return cleaned_asset\n",
    "\n",
    "\n",
    "def calculate_averages(\n",
    "    asset_data: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> DefaultDict[str, DefaultDict[str, np.float64]]:\n",
    "\n",
    "    summarised_data = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    for task_name, asset_df in asset_data.items():\n",
    "        for asset_name in asset_df.columns:\n",
    "            arrays_for_col = asset_df[asset_name].values\n",
    "            flattened_array = np.concatenate(arrays_for_col)\n",
    "            if flattened_array.size == 0:\n",
    "                print(f\"Empty array found for: {asset_name} in {task_name}\")\n",
    "            else:\n",
    "                mean = flattened_array.mean()\n",
    "                summarised_data[task_name][asset_name] = mean\n",
    "\n",
    "    return summarised_data\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    workspace_data = collect_asset_paths()\n",
    "    asset_data = load_asset_data(workspace_data=workspace_data)\n",
    "    summarised_data = calculate_averages(asset_data)\n",
    "    return summarised_data\n",
    "\n",
    "def transform_into_percentual_difference(asset_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in asset_df.columns:\n",
    "        if col != \"HTL\":\n",
    "            asset_df[col] = (asset_df[col] - asset_df[\"HTL\"]).mul(100)\n",
    "\n",
    "    return asset_df\n",
    "\n",
    "\n",
    "def filter_no_htl(asset_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    This is a helper function for the `create_comparison_df()` function. It is used to compare `No HTL` with `HTL`, so the filter condition looks for assets that contain `_no_htl` in their name.\n",
    "\n",
    "    Args:\n",
    "        asset_name (str): The asset to check for.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns `True` if the asset ends with `_no_htl`, else defaults to `False`.\n",
    "    \"\"\"\n",
    "    return asset_name.endswith(\"_no_htl\")\n",
    "\n",
    "\n",
    "def filter_random(asset_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    This is a helper function for the `create_comparison_df()` function. It is used to compare `Random (Filled Up)` with `HTL`, so the filter condition looks for assets that contain `_random` in their name.\n",
    "\n",
    "    Args:\n",
    "        asset_name (str): The asset to check for.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns `True` if the asset ends with `_random`, else defaults to `False`.\n",
    "    \"\"\"\n",
    "    return asset_name.endswith(\"_random\")\n",
    "\n",
    "\n",
    "def create_comparison_df(\n",
    "    data: DefaultDict[str, DefaultDict[str, np.float64]],\n",
    "    filter_condition: Callable[[str], bool],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comparison DataFrame by filtering asset names based on a given condition\n",
    "    and transforming them into percentual differences.\n",
    "\n",
    "    This function processes a nested dictionary structure where each task contains a dictionary\n",
    "    of assets and their corresponding values. For each task, it filters the asset names according\n",
    "    to the provided `filter_condition` function, cleans up asset names as needed, and collects\n",
    "    assets ending with \"HTL\". The resulting DataFrame for each task is transformed into percentual\n",
    "    differences and concatenated into a final DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (DefaultDict[str, DefaultDict[str, np.float64]]):\n",
    "            A nested dictionary where the outer key is the task name, the inner dictionary contains\n",
    "            asset names as keys and their values as floats.\n",
    "\n",
    "        filter_condition (Callable[[str], bool]):\n",
    "            A function that takes an asset name (string) and returns a boolean indicating whether\n",
    "            the asset should be included in the comparison. The function is responsible for determining\n",
    "            if an asset meets the filtering criteria.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            A concatenated DataFrame containing percentual differences for the filtered assets\n",
    "            across all tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    dfs: list[pd.DataFrame] = []\n",
    "    for task_name, asset_dict in data.items():\n",
    "        collected_assets = {}\n",
    "        for asset_name, asset_value in asset_dict.items():\n",
    "            if filter_condition(asset_name):\n",
    "                cleaned_asset = clean_up_asset_name(asset_name)\n",
    "                collected_assets[cleaned_asset] = asset_value\n",
    "            elif asset_name.endswith(\"HTL\"):\n",
    "                collected_assets[asset_name] = asset_value\n",
    "\n",
    "        asset_df = transform_into_percentual_difference(\n",
    "            asset_df=pd.DataFrame(data=collected_assets, index=[task_name])\n",
    "        )\n",
    "        dfs.append(asset_df)\n",
    "\n",
    "    merged_df = pd.concat(dfs)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    summarised_data = prepare_data()\n",
    "    data_filtered = create_comparison_df(summarised_data, filter_no_htl)\n",
    "    data_unfiltered = create_comparison_df(summarised_data, filter_random)\n",
    "\n",
    "    sns.set_theme()\n",
    "    sns.xkcd_palette\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        data=data_filtered[data_filtered.columns.difference([\"HTL\"])],\n",
    "        annot=True,\n",
    "        ax=axes[0],\n",
    "    )\n",
    "    axes[0].set_title(\"Filtered vs Unfiltered\")\n",
    "    sns.heatmap(\n",
    "        data=data_unfiltered[data_unfiltered.columns.difference([\"HTL\"])],\n",
    "        annot=True,\n",
    "        ax=axes[1],\n",
    "    )\n",
    "    axes[1].set_title(\"Random(Filled Up) vs Unfiltered\")\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03d3ef80819d8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ChatGPT Example for Above, can remove this cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the figure and axes for the boxplots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Incremental Boxplots\")\n",
    "\n",
    "# This list will hold different datasets for individual boxplots\n",
    "datasets = []\n",
    "\n",
    "# Assuming you have a mechanism to add datasets one at a time\n",
    "for i in range(1, 6):  # Example loop to simulate adding 5 datasets incrementally\n",
    "    # Simulate generating or loading a new dataset\n",
    "    new_data = np.random.normal(loc=i, scale=0.5, size=100)\n",
    "    datasets.append(new_data)  # Add the new dataset to the list\n",
    "\n",
    "# Clear the axes for fresh plot (optional if you want to redraw the boxplots)\n",
    "ax.clear()\n",
    "# Plot all the current datasets as individual boxplots\n",
    "ax.boxplot(datasets)\n",
    "ax.set_title(\"Incremental Boxplots\")\n",
    "plt.draw()  # Redraw the plot with the new data\n",
    "plt.pause(0.5)  # Pause to visually confirm the addition, adjust or remove as needed\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e9525",
   "metadata": {},
   "source": [
    "### Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9d191b06d81d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5.3 Significance Test from Paper\n",
    "\n",
    "# HTL mit NO HTL Vergleich & Random mit NO HTL vergleichen <-- Random hinzufügen (einfach assets laden)\n",
    "import deepsig\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def helper_function(significance_test_data: Dict[str, pd.DataFrame], filter_condition: Callable[[str], bool]) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    results = {}\n",
    "    for task_name, asset_df in significance_test_data.items():\n",
    "        collected_assets = {}\n",
    "        for col in asset_df.columns:\n",
    "            if filter_condition(col):\n",
    "                cleaned_asset = clean_up_asset_name(col)\n",
    "                flattened_array = np.concatenate(asset_df[col].values)\n",
    "                if flattened_array.size == 0:\n",
    "                    print(f\"Empty array found for: {col} in {task_name}\")\n",
    "                else:\n",
    "                    collected_assets[cleaned_asset] = flattened_array\n",
    "            elif col.endswith(\"HTL\"):\n",
    "                collected_assets[col] = np.concatenate(asset_df[col].values)\n",
    "\n",
    "        results[task_name] = collected_assets\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def prepare_significance_test_data(filter_condition: Callable[[str], bool]) -> Dict[str, pd.DataFrame]:\n",
    "    workspace_data = collect_asset_paths()\n",
    "    asset_data = load_asset_data(workspace_data=workspace_data)\n",
    "\n",
    "    return helper_function(asset_data, filter_condition)\n",
    "\n",
    "\n",
    "no_htl_vs_htl = prepare_significance_test_data(filter_no_htl)\n",
    "\n",
    "results = []\n",
    "for task_name, assets_dict in no_htl_vs_htl.items():\n",
    "    aso_test = {}\n",
    "    task_aso = []\n",
    "    htl_data = assets_dict[\"HTL\"]\n",
    "    for filter_strategy, no_htl_filter_data in assets_dict.items():\n",
    "        if filter_strategy != \"HTL\":\n",
    "            if (len(no_htl_filter_data) != seed_count * 30) or (len(htl_data) != seed_count * 30):\n",
    "                print(f\"Error! In {task_name} the filterstrategy '{filter_strategy}' has '{len(no_htl_filter_data)}' no HTL Data, and '{len(htl_data)}' HTL Data but expected were: '{seed_count * 30}'\")\n",
    "\n",
    "\n",
    "            # better = deepsig.aso(no_htl_filter_data, htl_data, num_jobs=8, seed=42)\n",
    "            # task_aso.append(better)\n",
    "\n",
    "#     assets_dict.pop(\"HTL\", None)\n",
    "#     results.append(pd.DataFrame(data=[task_aso], columns=assets_dict.keys(), index=[f\"{task_name}_htl_better\"]))\n",
    "\n",
    "# df = pd.concat(results)\n",
    "# df.to_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5580e9",
   "metadata": {},
   "source": [
    "### Marked Samples Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def collect_asset_paths(ASSET_PATHS: Optional[Path] = Path(\"cache\", \"assets\", COMET_WORKSPACE)) -> DefaultDict[str, List]:\n",
    "    TASK_ASSET_MAP = defaultdict(list)\n",
    "    for path in ASSET_PATHS.glob('**/*'):\n",
    "        if path.name.endswith(\"_Marked_Samples.npy\"):\n",
    "            task_name = path.parent.parent.name\n",
    "            if task_name != COMET_WORKSPACE:\n",
    "                TASK_ASSET_MAP[task_name].append(path)\n",
    "\n",
    "    return TASK_ASSET_MAP\n",
    "\n",
    "def load_asset_data(workspace_data: DefaultDict[str, List[Path]]) -> Dict[str, pd.DataFrame]:\n",
    "    results = {}\n",
    "    for task_name, assets in workspace_data.items():\n",
    "        collected_dfs = defaultdict(list)\n",
    "\n",
    "        for asset_path in assets:\n",
    "            data = np.load(asset_path)\n",
    "            if data.size == 0:\n",
    "                print(f\"{asset_path} is empty\")\n",
    "\n",
    "            filter_strategy_name = filter_names_clean[asset_path.name.replace(\"_Marked_Samples.npy\", \"\")]\n",
    "            collected_dfs[filter_strategy_name].append(data)\n",
    "\n",
    "\n",
    "        results[task_name] = pd.DataFrame(collected_dfs)\n",
    "    return results\n",
    "\n",
    "def group_by_filter_strategy(paths: List[Path]) -> DefaultDict[str, List[Path]]:\n",
    "    collected_dfs = defaultdict(list)\n",
    "    for asset_path in paths:\n",
    "        filter_strategy_name = filter_names_clean[asset_path.name.replace(\"_Marked_Samples.npy\", \"\")]\n",
    "        collected_dfs[filter_strategy_name].append(asset_path)\n",
    "\n",
    "    return collected_dfs\n",
    "\n",
    "def load_asset_data_for_coefficients(workspace_data: DefaultDict[str, List[Path]]) -> Dict[str, pd.DataFrame]:\n",
    "    results = {}\n",
    "    for task_name, assets in workspace_data.items():\n",
    "\n",
    "        collected_dfs = group_by_filter_strategy(assets)\n",
    "\n",
    "        tmp = []\n",
    "        for strategy, paths in collected_dfs.items():\n",
    "            seeds = []\n",
    "            seed_data = []\n",
    "            for path in sorted(paths):\n",
    "                data = np.load(path)\n",
    "                seed = int(path.parent.stem)\n",
    "\n",
    "                seeds.append(seed)\n",
    "                seed_data.append(data)\n",
    "\n",
    "            df = pd.DataFrame(data={strategy : seed_data}, index=seeds)\n",
    "\n",
    "            tmp.append(df)\n",
    "\n",
    "\n",
    "        results[task_name] = pd.concat(tmp, axis=1)\n",
    "    return results\n",
    "\n",
    "def check_for_null(df: pd.DataFrame, strategy1: str, strategy2: str) -> bool:\n",
    "    return pd.isnull(df.loc[strategy1, strategy2])\n",
    "\n",
    "def update_coefficient_value(df: pd.DataFrame, strategy1: str, strategy2: str, jaccard_index: float):\n",
    "    if check_for_null(df, strategy1, strategy2):\n",
    "        df.loc[strategy1, strategy2] = jaccard_index\n",
    "    else:\n",
    "        df.loc[strategy1, strategy2] = (df.loc[strategy1, strategy2] + jaccard_index) / 2\n",
    "\n",
    "\n",
    "\n",
    "def calculate_jaccard(x1: np.ndarray, x2: np.ndarray) -> float:\n",
    "    intersection = np.intersect1d(x1, x2).size\n",
    "    union = np.union1d(x1, x2).size\n",
    "    return (intersection / union) if union > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_overlap(x1, x2):\n",
    "    intersection = np.intersect1d(x1, x2).size\n",
    "    min_set = min(x1.size, x2.size)\n",
    "    return (intersection / min_set) if min_set > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_coefficient_matrix(asset_data: Dict[str, pd.DataFrame], coefficient: Callable[[np.ndarray, np.ndarray], float]) -> pd.DataFrame:\n",
    "\n",
    "    total_seeds = np.array([42 + i for i in range(0, seed_count)])\n",
    "    strategies = asset_data[next(iter(asset_data))].columns\n",
    "    strategy_pairs = list(combinations(strategies, 2))\n",
    "    overlap_matrix = pd.DataFrame(index=strategies, columns=strategies)\n",
    "    for task_name, df in asset_data.items():\n",
    "        df_index = np.array(df.index)\n",
    "        diff = np.setdiff1d(total_seeds, df_index)\n",
    "        if diff.size > 0:\n",
    "            print(f\"For the task '{task_name}' in '{COMET_WORKSPACE}' are incomplete seed values, namely: {diff}\")\n",
    "\n",
    "        for index in df_index:\n",
    "            for strategy1, strategy2 in strategy_pairs:\n",
    "                coffiecient_index = coefficient(\n",
    "                    df.loc[index, strategy1],\n",
    "                    df.loc[index, strategy2]\n",
    "                )\n",
    "                update_coefficient_value(overlap_matrix, strategy1, strategy2, coffiecient_index)\n",
    "                update_coefficient_value(overlap_matrix, strategy2, strategy1, coffiecient_index)\n",
    "\n",
    "\n",
    "    np.fill_diagonal(overlap_matrix.values, 1)\n",
    "    overlap_matrix = overlap_matrix.astype(float)\n",
    "    return overlap_matrix\n",
    "\n",
    "def plot_data(data: pd.DataFrame, coefficient_name: str):\n",
    "    # Set up the plot in stairwell style\n",
    "    sns.set_theme(style=\"white\")\n",
    "    colors = sns.color_palette(\"YlGnBu\", as_cmap=True)  # Stairwell-style color palette\n",
    "\n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        data=data,           # Overlap matrix as data\n",
    "        cmap=colors,                   # Use stairwell style color map\n",
    "        annot=True,                    # Annotate cells with values\n",
    "        fmt=\".2f\",                     # Format annotations to 2 decimal places\n",
    "        annot_kws={\"size\": 8},         # Font size for annotations\n",
    "        linewidths=0.5,                # Thin borders for each cell\n",
    "        linecolor='grey',              # Grey borders to define the stairwell look\n",
    "        cbar_kws={'shrink': 0.5},      # Shrink color bar for fitting\n",
    "        square=True                    # Square cells for a structured look\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(f\"{coefficient_name} Coefficient Heatmap for Filter Strategies\", fontsize=12, pad=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def jaccard_coefficient(asset_data: Dict[str, pd.DataFrame]):\n",
    "    overlap_matrix = calculate_coefficient_matrix(asset_data, calculate_jaccard)\n",
    "    plot_data(overlap_matrix, \"Jaccard\")\n",
    "\n",
    "def overlap_coefficient(asset_data: Dict[str, pd.DataFrame]):\n",
    "    overlap_matrix = calculate_coefficient_matrix(asset_data, calculate_overlap)\n",
    "    plot_data(overlap_matrix, \"Overlap\")\n",
    "\n",
    "\n",
    "def average_marked_samples() -> None:\n",
    "    workspace_data = collect_asset_paths()\n",
    "    marked_samples = load_asset_data(workspace_data=workspace_data)\n",
    "\n",
    "    dfs = []\n",
    "    for task_name, df in marked_samples.items():\n",
    "        collected_assets = {}\n",
    "        for col in df.columns:\n",
    "            avg_marked = np.concatenate(df[col].values).size / df[col].size\n",
    "            collected_assets[col] = avg_marked\n",
    "\n",
    "        asset_df = pd.DataFrame(data=collected_assets, index=[task_name])\n",
    "        dfs.append(asset_df)\n",
    "\n",
    "    merged_df = pd.concat(dfs)\n",
    "\n",
    "    sns.set_theme()\n",
    "    colors = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        data=merged_df,\n",
    "        cmap=colors,                   # Use stairwell style color map\n",
    "        annot=True,                    # Annotate cells with values\n",
    "        fmt=\".2f\",                     # Format annotations to 2 decimal places\n",
    "        annot_kws={\"size\": 8},         # Font size for annotations\n",
    "        linewidths=0.5,                # Thin borders for each cell\n",
    "        linecolor='grey',              # Grey borders to define the stairwell look\n",
    "        cbar_kws={'shrink': 0.5},      # Shrink color bar for fitting\n",
    "    )\n",
    "\n",
    "    # Adjust the layout to make boxes fit better\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(f\"Average Marked Samples for each Filter Strategy over {seed_count} seeds.\", fontsize=12, pad=10)  # You can change the title, font size, and padding\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "x = collect_asset_paths()\n",
    "y = load_asset_data(x)\n",
    "jaccard_coefficient(y)\n",
    "overlap_coefficient(y)\n",
    "average_marked_samples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
